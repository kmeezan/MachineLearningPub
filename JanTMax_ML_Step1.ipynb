{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JanTMax_ML_Step1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github.com/kmeezan/MachineLearningPub/blob/master/JanTMax_ML_Step1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c_Ya1ETi1RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_JdHP3Vi82-",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "This is an introduction to Machine Learning using geospatial data. It is based on the Machine Learning Basic Regression tutorials presented by Google Colab.  The [base example](https://https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_regression.ipynb) shows machine learning to predict fuel efficiency  of cars.\n",
        "\n",
        "\n",
        "##Geospatial Applications\n",
        "\n",
        " This project has modified  the base example to demonstrate basic Machine Learning techniques using Geospatial data and to provide a hands on introduction to Machine Learning tools.\n",
        " \n",
        " \n",
        "\n",
        "  <a target=\"_blank\" href=\"https://github.com/kmeezan/MachineLearningPub/blob/master/JanTMax_ML_Step1.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKcM8kyYMK8T",
        "colab_type": "text"
      },
      "source": [
        "##How to use this project\n",
        "This project provides a guided introduction to Machine Learning and Neural Networks using Tensor Flow via the Google Colab platform. Start by reading and running each cell in the notebook, then use the guided learning companion documentation to explore the structure of the neural network and swap in your own data set.\n",
        "\n",
        "To run the project, click on the play button in each code cell. Cells must be run sequentially. You can restart the runtime from the Runtime pulldown menu.\n",
        "\n",
        "###About Colab and TensorFlow\n",
        "[TensorFlow](https://www.tensorflow.org/) is the dominant deep learning tool for Data Scientists. TensorFlow can run as a stand-alone software, but you need to have a fairly robust computer to run it efficiently. Colab is a Google Cloud solution that allows you to run TensorFlow in the cloud (via your Google account). \n",
        "\n",
        "For this project, you should have a Google account and (if you want to run a project with your own data set) a GitHub account. For an easy introduction to GitHub, see[ this tutorial.](https://guides.github.com/activities/hello-world/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3upjC08Ljh6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q_lmSGmjvQK",
        "colab_type": "text"
      },
      "source": [
        "# Project Overview\n",
        "This notebook uses surface temperature readings collected from National Weather Service stations in the conterminous  United States. Latitude, longitude and elevation are used to predict maximum surface temperature (for January, 2019). \n",
        "\n",
        "# Geospatial Technology and Machine Learning\n",
        "There are two types of predictions in Machine Learning. A regression problem predicts a continuous numeric value such as a price or a probability. A classification problem selects a class from a list of classes (for example, where a picture contains cat or a dog, recognizing and classifying which animal is in the picture).\n",
        "\n",
        "This notebook is intended as a model for how machine learning can work with spatially distributed data sets. We are going to examine a regression problem (predicting surface temperature) in this notebook.\n",
        "\n",
        "This example uses the tf.keras API, see[ this guide](https://www.tensorflow.org/guide/keras) for details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBmKK2hfgOX",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Intelligence, Machine Learning & Deep Learning: A short introduction\n",
        "##AI, ML and DL: What is the difference?\n",
        "The terms Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL) are often used togther, and the divisions between the three are not well defined. Here is a short comparison:\n",
        "\n",
        "![Venn diagram showing AI on the outermost circle, with ML inside and DL inside ML](https://raw.githubusercontent.com/kmeezan/ML_surfaceTemp/master/venn_AI_ML_DL2.png)\n",
        "\n",
        "###Artificial Intelligence (AI)\n",
        "AI is the umbrella term that encompasses both the simple, rules based algorithms that use if/then statements, as well as the more complex algorithms that self-improve with exposure to additional data (Machine Learning) and Neural Networks that strive to mimic the way brains work by 'self teaching' rules based on vast amounts of data (Deep Learning). Simple AI has been present from the beginning of computers.\n",
        "\n",
        "###Machine Learning (ML) vs. Deep Learning (DL)\n",
        "Machine learning and Deep learning are both subsets of AI. Both apply computer algorithms to identify patterns in data sets (as opposed to the humans providing the 'rules' in simple AI).\n",
        "Deep learning is the newest addition to the AI family. It uses artificial neural networks to mimic the learning process that takes place in a brain. Deep learning can discover patterns in data and identify the key parameters needed to predict results. Deep learning requires very large amounts of data and massive computing power to be effective.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVU-TimNre9U",
        "colab_type": "text"
      },
      "source": [
        "##Install modules to run the neural network\n",
        "The first thing we need to do is install the modules that we need to run our model. TensorFlow uses the programming language Python, and we are going to install and import several 'add on' modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIfo_IeskybL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQ_7L8ok6D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import modules \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('classic')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjNDXCKRlBvR",
        "colab_type": "text"
      },
      "source": [
        "# Get the data\n",
        "In this step, you will download the data set that the model will use. In this case, the data is housed in a file on GitHub. \n",
        "\n",
        "Three good methods to access CSV files in CoLab can be found [here](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92). This is an excerpt that was applied to create the snippit below: The easiest way to upload a CSV file is from your GitHub repository. Click on the dataset in your repository, then click on View Raw. Copy the link to the raw dataset and store it as a string variable called url in Colab. Note that the GitHub file needs to be public for this to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "irLd-nTZUHU3",
        "colab": {}
      },
      "source": [
        "# link to data on GitHub\n",
        "url = 'https://raw.githubusercontent.com/kmeezan/ML_surfaceTemp/master/Jan_2019TMax_full.csv'\n",
        "\n",
        "\n",
        "# print the first 10 lines of the file to confirm\n",
        "print(pd.read_csv(url, nrows=10))\n",
        "\n",
        "\n",
        "# access the data in a Pandas Dataframe\n",
        "dataset = pd.read_csv(url)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnORjA44BHr",
        "colab_type": "text"
      },
      "source": [
        "#Clean the data for use with the model\n",
        "Nearly all data will need to be manipulated in some way in order for you to use it in a TensorFlow model. The data we are using has been pre-processed by removing unnecessary fields and saving it in a CSV format. We are now going to do some additional pre-processing within this notebook.\n",
        "## Split the data into train and test\n",
        "\n",
        "The data will be split into two groups: The 'training' set will be used to train the neural network. The remaining 'test' set will be reserved to test our model and see how well it learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMvQLHKD4D8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "print(\"The length of the training set is: \", len(train_dataset), \n",
        "      \"\\nThe length of the testing data set is: \", len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_3gj5653dz",
        "colab_type": "text"
      },
      "source": [
        "# Inspect the data\n",
        "Our data set has the variables latitude, longitude, elevation and temperature (maximum average temperature for January 2019). Let's take a quick look at the joint distribution of a few pairs of columns from the training set to get an idea of what we are dealing with.\n",
        "\n",
        "We will use `seaborn pairplot` to graph each variable against the other. The line plots are actually histograms showing the frequency of the data (latitude vs. latitude, etc), the remainder are scatter plots.\n",
        "\n",
        "Note that the Latitude/Longitude pairing ends up looking like an outline of the USA (which it should!). Do you notice any other patterns in the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy_6n4QP5JqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(train_dataset[[\"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"TMAX\"]], diag_kind=\"kde\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiEGDT9y65Qp",
        "colab_type": "text"
      },
      "source": [
        "Next, let's look at some of the statistics of the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGvD3E26xn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"TMAX\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJA1ZmPv7NTC",
        "colab_type": "text"
      },
      "source": [
        "## Split features from labels\n",
        "\n",
        "The next step in pre-processing our data is to separate the target value, or \"label\", from the features that we are going to use to predict the target value. This label is the value that you will train the model to predict.\n",
        "\n",
        "In this case, our 'label' is 'TMAX' or the maximum average monthly temperature recorded at each weather station in the month of January, 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKPsyeP07QRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_dataset.pop('TMAX')\n",
        "test_labels = test_dataset.pop('TMAX')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBQxUtDSPHI",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the data\n",
        "\n",
        "Look again at the train_stats block above and note the broad range of values for each data type.\n",
        "\n",
        "It is good practice to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
        "\n",
        "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ZE15IzSRnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSDNba6UScD6",
        "colab_type": "text"
      },
      "source": [
        "This normalized data is what we will use to train the model.\n",
        "\n",
        "*Caution*: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model. That includes the test set as well as live data when the model is used in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2eslH0SoO8",
        "colab_type": "text"
      },
      "source": [
        "#Build the model\n",
        "Now it is time to build our model. We will be creating an artificial  neural network (an example of Deep Learning)\n",
        "\n",
        "##About neural networks\n",
        "![graphic of a neural network](https://cdn-images-1.medium.com/max/1600/0*hzIQ5Fs-g8iBpVWq.jpg)\n",
        "\n",
        "### What is a neural network?\n",
        "A neural network consists of an input layer of data, a series of 'hidden layers' and an output layer, or prediction. The hidden layers have a series of 'nodes' that contain algorithms that help to refine the prediction of the network from a pure guess (in its first run), with each run-through of the network refining the accuracy of the guess. This [TensorFlow playground site ](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.18915&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=false&ySquared=false&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false)is a fun way to visualize how a neural network functions.\n",
        "\n",
        "###The input layer\n",
        "The input layer is the data you input to train your network. In our case, the input layer consists of latitude, longitude and elevation. We are training these against the 'labels' of maximum temperature which we split off earlier.\n",
        "\n",
        "###Hidden layers\n",
        "A hallmark of a neural network is one or more hidden layers (we have two in this example). The layers are made up of 'neurons' that help the network learn. Each neruon has an activation function assigned to it. In this network we are using ` relu.` For more on activation functions, see [this article](https://theffork.com/activation-functions-in-neural-networks/). The network starts out with random guesses for the values, then compares them to the actual values (the training labels). It then passes the guesses on to the next layer to be further refined. There are many different versions of hidden layers that refine the data in different ways. \n",
        "\n",
        "`Keras` is the neural network model used by TensorFlow. We will be using 'dense' type hidden layers.\n",
        "\n",
        "###Epochs\n",
        "The epochs come into play when we run the model. Epochs are the number of times that the network reviews the data and refines its predictions. The network refines its guesses with each 'epoch'\n",
        "\n",
        "##Building our model\n",
        "Now let's build our model. Here, we'll use a Sequential model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, build_model, since we'll create a second model, later on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3xUbQstTPWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a function to build the model with keras.Sequential using 2 dense layers with relu activation.\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(64, activation=tf.nn.relu),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-DHJooTYBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c--j_jbMU56H",
        "colab_type": "text"
      },
      "source": [
        "## Inspect the model\n",
        "\n",
        "Use the` .summary` method to print a simple description of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLbFMytUwXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7l7_mSVEA2",
        "colab_type": "text"
      },
      "source": [
        "Now try out the model. Take a batch of 10 examples from the training data and call ` model.predict` on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DfK3a-_VHuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch = normed_train_data[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZEylJuyVQoD",
        "colab_type": "text"
      },
      "source": [
        "It seems to be working, and it produces a result of the expected shape and type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1qiQOf5VgLw",
        "colab_type": "text"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "Now it is time to train our model. We will set the *epochs* to 500 to start with. \n",
        "\n",
        "We will train the model and record its training and validation accuracy in the `history` object. It can take some time to train a model, so to help us visualize the training progress, CoLab will produce a graphic printout of the training progress with dots (.  .  .  .  )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Ne7lHEVrr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 100 == 0: print('')\n",
        "    if epoch % 500 == 0: print('*')\n",
        "    print('.', end='')\n",
        "\n",
        "#Epochs are the number of training cycles the model will run through    \n",
        "EPOCHS = 500\n",
        "\n",
        "history = model.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[PrintDot()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPMQA6PoXjPe",
        "colab_type": "text"
      },
      "source": [
        "Visualize the model's training progress using the statistics stored in the `history` object. We will first look at the end of the data printout, then graph the results of the entire training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVlwiexiXkOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8xjHwTsX3zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  sns.set(style='whitegrid')\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [TMAX]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.title('Mean Absolute Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error [TMAX]')\n",
        "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
        "           label='Train Error')\n",
        "  plt.title('Mean Squared Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSkxx1msZ37R",
        "colab_type": "text"
      },
      "source": [
        "## The risks of over training\n",
        "This graph shows little improvement, or even degradation in the validation error after about 100 epochs. This is a common problem in Machine Learning, especially with a small(ish) dataset: the computer basically memorizes the data. The result is that when presented with a new set of data, it actually does worse because it is trying to fit the new data into what it has memorized, rather than simply applying rules that it has learned.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQFFvNHVBBZr",
        "colab_type": "text"
      },
      "source": [
        "##Adding patience to our model\n",
        "In this step we will revise our model by adding a 'patience' parameter to monitor the  validation score. Let's update the `model.fit` call to automatically stop training when the validation score doesn't improve. We'll use an EarlyStopping callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training.\n",
        "\n",
        "You can learn more about this callback  [here](https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG--gYohZ9gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
        "sns.set(style='whitegrid')\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7c4m6NFdNAm",
        "colab_type": "text"
      },
      "source": [
        "The graph shows that on the validation set, the average error is usually around +/- 2 degrees fahrenheit. Is this good? We'll leave that decision up to you.\n",
        "\n",
        "Let's see how well the model generalizes by using the test set, which we did not use when training the model. The test data is a completely fresh set of data points that we will present to the model. This tells us how well we can expect the model to predict when we use it in the real world.\n",
        "\n",
        "Note that if you run this model multiple times, you will get slightly different values for the Mean Absolute Error. This is because the model learns slightly differently each time!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQ7ybI-dXoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} Degrees F = TMAX\".format(mae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPl2oxxndqz2",
        "colab_type": "text"
      },
      "source": [
        "#Test the model\n",
        "Finally, it is time to test our model and view the results.\n",
        "##Make predictions\n",
        "\n",
        "Predict TMAX values using data in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWyzMEUzdtn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "#print(normed_test_data)\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.scatter(x=test_labels, y=test_predictions, s=70)\n",
        "plt.xlabel('True Values [TMAX]')\n",
        "plt.ylabel('Predictions [TMAX]')\n",
        "plt.title('Model Accuracy')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519kI0OOg4NS",
        "colab_type": "text"
      },
      "source": [
        "Reverse the normalization of the data so we can plot latitude & longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0KQRx9zg9qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def de_norm(x):\n",
        "  return (normed_test_data * train_stats['std'])+(train_stats['mean'])\n",
        "  \n",
        "de_normed_test_data = de_norm(normed_test_data)\n",
        "\n",
        "# un-comment the following line to check that this process worked\n",
        "# print(de_normed_test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8f0-RIg4FiP",
        "colab_type": "text"
      },
      "source": [
        "#Plot the data\n",
        "Let's look at the testing data points and compare actual values to the values predicted by the model.\n",
        "##Actual & Predicted temperature values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSfYEQgQptp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setup the master plot\n",
        "fig = plt.figure(figsize=plt.figaspect(.4))\n",
        "\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "#fig, axs = plt.subplots(1, 2)\n",
        "fig.suptitle(\"January Temperature (F)\", fontsize=16)\n",
        "\n",
        "# ---- Plot Actual ----\n",
        "ax.scatter(x=de_normed_test_data[\"LONGITUDE\"], y=de_normed_test_data[\"LATITUDE\"], \n",
        "                c=test_labels, cmap='RdYlBu_r', s=70, vmin=10, vmax=80)\n",
        "ax.set_title(\"Actual\")\n",
        "\n",
        "# ---- Plot Predicted ----\n",
        "ax2 = fig.add_subplot(1,2,2)\n",
        "ax2.set_title(\"Predicted\")\n",
        "\n",
        "my_color_bar = ax2.scatter(x=de_normed_test_data[\"LONGITUDE\"], \n",
        "                          y=de_normed_test_data[\"LATITUDE\"], c=test_predictions, \n",
        "                          cmap='RdYlBu_r', s=80, vmin=10, vmax=80)\n",
        "# setup color bar\n",
        "cb_ax = fig.add_axes([0.93, 0.1, 0.02, 0.8])\n",
        "cbar = fig.colorbar(my_color_bar, cax=cb_ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZlu10qaku0",
        "colab_type": "text"
      },
      "source": [
        "##Error\n",
        "Next, let's look at the error for each data point as calculated by subtracting the actual minus the predicted value. Are some regions more prone to error than others?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT2_AXOLYmYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(style='whitegrid')\n",
        "plot = plt.scatter(x=de_normed_test_data[\"LONGITUDE\"], y=de_normed_test_data[\"LATITUDE\"], \n",
        "                c=(test_labels - test_predictions), cmap='BrBG')\n",
        "plt.clf()\n",
        "plt.colorbar(plot)\n",
        "\n",
        "ax = plt.scatter(x=de_normed_test_data[\"LONGITUDE\"], y=de_normed_test_data[\"LATITUDE\"], \n",
        "                c=(test_labels - test_predictions), cmap='BrBG', s=70)\n",
        "\n",
        "\n",
        "plt.title(\"Error: Actual - Predicted values\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u_-dD-yd5l8",
        "colab_type": "text"
      },
      "source": [
        "It looks like our model predicts reasonably well. Let's take a look at the error distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVa7MG4Nd6rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [TMAX]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEgeBiNKeFGh",
        "colab_type": "text"
      },
      "source": [
        "It's not quite gaussian, but we might expect that because the number of samples is very small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLQOpf53eGk_",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "\n",
        "This notebook introduced a few techniques to handle a regression problem.\n",
        "\n",
        "\n",
        "*    Mean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\n",
        "*   Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n",
        "*   When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
        "*   If there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.\n",
        "*   Early stopping is a useful technique to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ]
    }
  ]
}
