{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USA_Tmax_ML_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmeezan/MachineLearningPub/blob/master/USA_Tmax_ML_Step3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c_Ya1ETi1RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_JdHP3Vi82-",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "This is the third  in a series introducing Machine Learning using geospatial data. This project adds the variable of time into the model and expands it over a 20 year period for 4000 weather stations in the continental USA. The overall project is based on the Machine Learning Basic Regression tutorials presented by Google Colab.  The [base example](https://https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_regression.ipynb) shows machine learning to predict fuel efficiency  of cars.\n",
        "\n",
        "## Modifications to this project\n",
        "This project builds on the  base Machine Learning with Tensor Flow on Google CoLabs project ([link to GitHub code](https://https://github.com/kmeezan/ML_surfaceTemp/blob/master/ShareCopyNN_JanTMax.ipynb)). This project adds the variable of *time*. Input variables are latitude, longitude, elevation and month (1-12), with the predicted value of TMAX or maximum temperature. By adding the variable of time, this project introduces some additional complexities in the visualization of the data by revising the visualization to four dimensions (latitude, longitude, time and temperature), which are rendered as three dimensional scatter plots. This project presents a methodology to slice the test data set by time and visualize the results as two dimensional scatter plots (latitude, longitude and temperature)  by time slice.\n",
        "\n",
        "\n",
        "  <a target=\"_blank\" href=\"https://github.com/kmeezan/ML_surfaceTemp/blob/master/ShareCopyNN_OregonTMAX.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXoIJ4HWVLm4",
        "colab_type": "text"
      },
      "source": [
        "##Geospatial Applications\n",
        "\n",
        " This project has modified the base example to demonstrate basic Machine Learning techniques using Geospatial data and to provide a hands on introduction to Machine Learning tools.\n",
        "\n",
        "###Steps to get here\n",
        " \n",
        "This is the third in a series of Machine Learning using Geospatial Data and Tensor Flow in CoLabs. This notebook utilizes a very large dataset (800,000+ data points) that can take over 1 hour to run in Google Colabs. Therefore, the process for this ML project was built in two smaller test cases:\n",
        "\n",
        "\n",
        "*   Single month/year data set: [January 2019 TMax](https://github.com/kmeezan/MachineLearningPub/blob/master/FinalJanTemperatureML_experiment_FullData.ipynb)\n",
        "*   Space/time data for 6 years: [Oregon 2012-2018](https://github.com/kmeezan/MachineLearningPub/blob/master/JanTMax_ML_Step1.ipynb)\n",
        "\n",
        "This notebook synthesizes the spatial elements of the first notebook with the time elements of the second.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKcM8kyYMK8T",
        "colab_type": "text"
      },
      "source": [
        "##How to use this project\n",
        "To use this notebook, start by reading and running each cell in the notebook, then use the guided learning companion documentation to explore the structure of the neural network and swap in your own data set.\n",
        "\n",
        "**To run the project**, click on the play button in each code cell. Cells must be run sequentially. You can restart the runtime from the Runtime pulldown menu.\n",
        "\n",
        "###About Colab and TensorFlow\n",
        "[TensorFlow](https://www.tensorflow.org/) is the dominant deep learning tool for Data Scientists. TensorFlow can run as a stand-alone software, but you need to have a fairly robust computer to run it efficiently. Colab is a Google Cloud solution that allows you to run TensorFlow in the cloud (via your Google account). \n",
        "\n",
        "For this project, you should have a Google account and (if you want to run a project with your own data set) a GitHub account. For an easy introduction to GitHub, see[ this tutorial.](https://guides.github.com/activities/hello-world/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3upjC08Ljh6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q_lmSGmjvQK",
        "colab_type": "text"
      },
      "source": [
        "# Project Overview\n",
        "This notebook uses monthly average maximum surface temperature readings collected from National Weather Service stations in the contiental United States for the period 1998 to 2019. Therefore each spatial location (latitude/longitude pair) has approximately 20 training points replicating latitude, longitude, elevation, month and temperature. Some data points were dropped because they did not contain a temperature value (Null). The data set consists of over 800,000 data points covering 4000 weather stations.\n",
        "\n",
        "## Geospatial Technology and Machine Learning\n",
        "There are two types of predictions in Machine Learning. A regression problem predicts a continuous numeric value such as a price or a probability. A classification problem selects a class from a list of classes (for example, where a picture contains cat or a dog, recognizing and classifying which animal is in the picture).\n",
        "\n",
        "This notebook is intended as a model for how machine learning can work on a regression problem with spatially distributed data sets over time. We are going to examine a regression problem (predicting surface temperature) in this notebook.\n",
        "\n",
        "This example uses the tf.keras API, see[ this guide](https://www.tensorflow.org/guide/keras) for details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4TEMqZ0_cl",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Intelligence, Machine Learning & Deep Learning: A short introduction\n",
        "##AI, ML and DL: What is the difference?\n",
        "The terms Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL) are often used togther, and the divisions between the three are not well defined. Here is a short comparison:\n",
        "\n",
        "![Venn diagram showing AI on the outermost circle, with ML inside and DL inside ML](https://raw.githubusercontent.com/kmeezan/ML_surfaceTemp/master/venn_AI_ML_DL2.png)\n",
        "\n",
        "###Artificial Intelligence (AI)\n",
        "AI is the umbrella term that encompasses both the simple, rules based algorithms that use if/then statements, as well as the more complex algorithms that self-improve with exposure to additional data (Machine Learning) and Neural Networks that strive to mimic the way brains work by 'self teaching' rules based on vast amounts of data (Deep Learning). Simple AI has been present from the beginning of computers.\n",
        "\n",
        "###Machine Learning (ML) vs. Deep Learning (DL)\n",
        "Machine learning and Deep learning are both subsets of AI. Both apply computer algorithms to identify patterns in data sets (as opposed to the humans providing the 'rules' in simple AI).\n",
        "Deep learning is the newest addition to the AI family. It uses artificial neural networks to mimic the learning process that takes place in a brain. Deep learning can discover patterns in data and identify the key parameters needed to predict results. Deep learning requires very large amounts of data and massive computing power to be effective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVU-TimNre9U",
        "colab_type": "text"
      },
      "source": [
        "#Install modules to run the neural network\n",
        "The first thing we need to do is install the modules that we need to run our model. TensorFlow uses the programming language Python, and we are going to install and import several 'add on' modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIfo_IeskybL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQ_7L8ok6D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import modules \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('classic')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjNDXCKRlBvR",
        "colab_type": "text"
      },
      "source": [
        "# Get the data\n",
        "In this step, you will download the data set that the model will use. In this case, the data is housed in a file on GitHub. \n",
        "\n",
        "Three good methods to access CSV files in CoLab can be found [here](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92). This is an excerpt that was applied to create the snippit below: The easiest way to upload a CSV file is from your GitHub repository. Click on the dataset in your repository, then click on View Raw. Copy the link to the raw dataset and store it as a string variable called url in Colab. Note that the GitHub file needs to be public for this to work.\n",
        "\n",
        "## Problems with big files\n",
        "This data set is extremely large (over 800,000 data points). The methodology for this notebook was first built on a much smaller subset of data: \n",
        "\n",
        "\n",
        "*   Single month/year data set:[ January 2019 TMax](https://github.com/kmeezan/MachineLearningPub/blob/master/JanTMax_ML_Step1.ipynb)\n",
        "*   Space/time data for 6 years: [Oregon 2012-2018](https://github.com/kmeezan/MachineLearningPub/blob/master/Oregon_Temperature_Time_Step2.ipynb)\n",
        "\n",
        "The CSV file size for this project was too large to upload to GitHub, so it has been broken into two files (part of the Python pre-processing) and then reassembled into a single Pandas dataframe in this step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "irLd-nTZUHU3",
        "colab": {}
      },
      "source": [
        "# link to data on GitHub\n",
        "url = 'https://raw.githubusercontent.com/kmeezan/ML_surfaceTemp/master/tmax_20_usa.csv'\n",
        "url2 = 'https://raw.githubusercontent.com/kmeezan/ML_surfaceTemp/master/tmax_20_usa2.csv'\n",
        "\n",
        "# convert csv to panda\n",
        "csv_to_pd1 = pd.read_csv(url)\n",
        "csv_to_pd2 = pd.read_csv(url2)\n",
        "\n",
        "\n",
        "# merge the two files into one\n",
        "dataset = csv_to_pd1.append(csv_to_pd2, ignore_index=True)\n",
        "\n",
        "\n",
        "# print the first 10 lines of the file to confirm\n",
        "print(dataset.head(10))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnORjA44BHr",
        "colab_type": "text"
      },
      "source": [
        "#Clean the data for use with the model\n",
        "Nearly all data will need to be manipulated in some way in order for you to use it in a TensorFlow model. The data we are using has been pre-processed with Python prior to being uploaded to GitHub.\n",
        "\n",
        "The source data for this project downloaded from [NOAA](https://www.ncei.noaa.gov/data/gsom/). The data set was a total of 14.5GB of individual CSV files corresponding to weather stations around the world. Python was used to filter latitude and longitude parameters and select a random list of 4000 stations in the continental USA which had data readings in the past 20 years. A Python program then pulled the relevant data from the selected stations, filtered out for null values and output the cleaned CSV files posted on GitHub.\n",
        "\n",
        "We are now going to do some additional pre-processing within this notebook to ready the project for the neural network.\n",
        "## Split the data into train and test\n",
        "\n",
        "The data will be split into two groups: The 'training' set will be used to train the neural network. The remaining 'test' set will be reserved to test our model and see how well it learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMvQLHKD4D8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "print(\"The length of the training set is: \", len(train_dataset), \n",
        "      \"\\nThe length of the testing data set is: \", len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_3gj5653dz",
        "colab_type": "text"
      },
      "source": [
        "# Inspect the data\n",
        "Our data set has the variables latitude, longitude, elevation and temperature (maximum average maximum temperature for the month for the years 1998-2019). Let's take a quick look at the joint distribution of a few pairs of columns from the training set to get an idea of what we are dealing with.\n",
        "\n",
        "We will use `seaborn pairplot` to graph each variable against the other. The line plots are actually histograms showing the frequency of the data (latitude vs. latitude, etc), the remainder are scatter plots.\n",
        "\n",
        "Note that the Latitude/Longitude pairing ends up looking like an outline of the continental USA (which it should!). Do you notice any other patterns in the data? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy_6n4QP5JqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# note we are plotting the test data set because it is much smaller and will plot faster\n",
        "sns.pairplot(test_dataset[[\"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"TMAX\"]], diag_kind=\"kde\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiEGDT9y65Qp",
        "colab_type": "text"
      },
      "source": [
        "Next, let's look at some of the statistics of the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGvD3E26xn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"TMAX\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJA1ZmPv7NTC",
        "colab_type": "text"
      },
      "source": [
        "## Split features from labels\n",
        "\n",
        "The next step in pre-processing our data is to separate the target value, or \"label\", from the features that we are going to use to predict the target value. This label is the value that you will train the model to predict.\n",
        "\n",
        "In this case, our 'label' is 'TMAX' or the maximum average monthly temperature recorded at each weather station."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKPsyeP07QRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_dataset.pop('TMAX')\n",
        "test_labels = test_dataset.pop('TMAX')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBQxUtDSPHI",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the data\n",
        "\n",
        "Look again at the train_stats block above and note the broad range of values for each data type.\n",
        "\n",
        "It is good practice to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
        "\n",
        "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ZE15IzSRnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSDNba6UScD6",
        "colab_type": "text"
      },
      "source": [
        "This normalized data is what we will use to train the model.\n",
        "\n",
        "*Caution*: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model. That includes the test set as well as live data when the model is used in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2eslH0SoO8",
        "colab_type": "text"
      },
      "source": [
        "#Build the model\n",
        "Now it is time to build our model. We will be creating an artificial  neural network (an example of Deep Learning)\n",
        "\n",
        "##About neural networks\n",
        "A neural network consists of an input layer of data, a series of 'hidden layers' and an output layer, or prediction. The hidden layers have a series of 'nodes' that contain algorithms that help to refine the prediction of the network from a pure guess (in its first run), with each run-through of the network refining the accuracy of the guess. For more about neural networks, see the summary in the [*Predicting January Maximum Temperature*](https://github.com/kmeezan/MachineLearningPub/blob/master/JanTMax_ML_Step1.ipynb) notebook.\n",
        "\n",
        "`Keras` is the neural network model used by TensorFlow. We will be using a Sequential model with 'dense' type hidden layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3xUbQstTPWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a function to build the model with keras.Sequential using 2 dense layers with relu activation.\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(64, activation=tf.nn.relu),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-DHJooTYBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c--j_jbMU56H",
        "colab_type": "text"
      },
      "source": [
        "## Inspect the model\n",
        "\n",
        "Use the` .summary` method to print a simple description of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLbFMytUwXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7l7_mSVEA2",
        "colab_type": "text"
      },
      "source": [
        "Now try out the model. Take a batch of 10 examples from the training data and call ` model.predict` on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DfK3a-_VHuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch = normed_train_data[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZEylJuyVQoD",
        "colab_type": "text"
      },
      "source": [
        "It seems to be working, and it produces a result of the expected shape and type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1qiQOf5VgLw",
        "colab_type": "text"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "Now it is time to train our model. We will set the *epochs* to 100 to start with. \n",
        "\n",
        "We will train the model and record its training and validation accuracy in the `history` object. It can take some time to train a model, so to help us visualize the training progress, CoLab will produce a graphic printout of the training progress with dots (.  .  .  .  )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Ne7lHEVrr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if epoch % 10 == 0: print('')\n",
        "    if epoch % 50 == 0: print('*')\n",
        "    print('.', end='')\n",
        "\n",
        "#Epochs are the number of training cycles the model will run through    \n",
        "EPOCHS = 100\n",
        "\n",
        "history = model.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n",
        "  callbacks=[PrintDot()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPMQA6PoXjPe",
        "colab_type": "text"
      },
      "source": [
        "Visualize the model's training progress using the statistics stored in the `history` object. We will first look at the end of the data printout, then graph the results of the entire training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVlwiexiXkOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8xjHwTsX3zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  sns.set(style='whitegrid')\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [TMAX]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.title('Mean Absolute Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error [TMAX]')\n",
        "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
        "           label='Train Error')\n",
        "  plt.title('Mean Squared Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,10])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSkxx1msZ37R",
        "colab_type": "text"
      },
      "source": [
        "## The risks of over training\n",
        "This graph shows little improvement, or even degradation in the validation error after about 100 epochs. This is a common problem in Machine Learning, especially with a small(ish) dataset: the computer basically memorizes the data. The result is that when presented with a new set of data, it actually does worse because it is trying to fit the new data into what it has memorized, rather than simply applying rules that it has learned.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQFFvNHVBBZr",
        "colab_type": "text"
      },
      "source": [
        "##Adding patience to our model\n",
        "In this step we will revise our model by adding a 'patience' parameter to monitor the  validation score. Let's update the `model.fit` call to automatically stop training when the validation score doesn't improve. We'll use an EarlyStopping callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training.\n",
        "\n",
        "The patience parameter has been set to 10 in this model with a validation split of 0.2. This value was achieved through trial and error.\n",
        "\n",
        "You can learn more about this callback  [here](https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG--gYohZ9gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
        "sns.set(style='whitegrid')\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7c4m6NFdNAm",
        "colab_type": "text"
      },
      "source": [
        "The graph shows that on the validation set, the average error is usually around +/- 2 degrees fahrenheit. Is this good? We'll leave that decision up to you. Note that this accuracy of this model is less than the single month prediction in the*[ Predicting January Maximum Temperature](https://github.com/kmeezan/ML_surfaceTemp/blob/master/ShareCopyNN_JanTMax.ipynb)* notebook. This is due to the additional dimension of time.\n",
        "\n",
        "Let's see how well the model generalizes by using the test set, which we did not use when training the model. The test data is a completely fresh set of data points that we will present to the model. This tells us how well we can expect the model to predict when we use it in the real world.\n",
        "\n",
        "Note that if you run this model multiple times, you will get slightly different values for the Mean Absolute Error. This is because the model learns slightly differently each time!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQ7ybI-dXoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} Degrees F = TMAX\".format(mae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPl2oxxndqz2",
        "colab_type": "text"
      },
      "source": [
        "#Test the model\n",
        "Finally, it is time to test our model and view the results.\n",
        "##Make predictions\n",
        "\n",
        "Predict TMAX values using data in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWyzMEUzdtn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "# create an empty list to hold the index number of the test data\n",
        "test_keys = []\n",
        "for key, value in normed_test_data.items():\n",
        "  for k, v in value.items():\n",
        "    test_keys.append(k)\n",
        "\n",
        "# create a dictionary with the index number and predicted value for test data\n",
        "test_predictions_keysd =(dict(zip(test_keys, test_predictions)))\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.scatter(x=test_labels, y=test_predictions, s=70)\n",
        "plt.xlabel('True Values [TMAX]')\n",
        "plt.ylabel('Predictions [TMAX]')\n",
        "plt.title('Model Accuracy')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([-25,plt.xlim()[1]])\n",
        "plt.ylim([-25,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519kI0OOg4NS",
        "colab_type": "text"
      },
      "source": [
        "Reverse the normalization of the data so we can plot latitude & longitude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0KQRx9zg9qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def de_norm(x):\n",
        "  return (normed_test_data * train_stats['std'])+(train_stats['mean'])\n",
        "  \n",
        "de_normed_test_data = de_norm(normed_test_data)\n",
        "\n",
        "# un-comment the following line to check that this process worked\n",
        "# print(de_normed_test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8f0-RIg4FiP",
        "colab_type": "text"
      },
      "source": [
        "#Plot the data\n",
        "Let's look at the testing data points and compare actual values to the values predicted by the model.\n",
        "##Temperature values in 3D\n",
        "3D visualization of the data (location in 2D space vs. time (z-value) for temperature) is limiting. However, let's take a look at a 3D scatter plot of the data. Next, we will examine montly time slices in 2D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-idQL05bE6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  -- Actual values --\n",
        "sns.set(style='whitegrid')\n",
        "fig = plt.figure(figsize=plt.figaspect(.3))\n",
        "\n",
        "# set up the axes for the first plot\n",
        "ax = fig.add_subplot(1,2,1, projection='3d')\n",
        "\n",
        "\n",
        "ax.scatter(xs=de_normed_test_data[\"LONGITUDE\"], ys=de_normed_test_data[\"LATITUDE\"], \n",
        "                zs=de_normed_test_data[\"MONTH\"], c=test_labels, cmap='RdYlBu_r', s=40, linewidths=0)\n",
        "\n",
        "fig.suptitle(\"Temperature (C)\", fontsize=16)\n",
        "ax.set_title(\"Actual\", fontsize=14)\n",
        "\n",
        "# setup color bar\n",
        "my_color_bar = ax.scatter(xs=de_normed_test_data[\"LONGITUDE\"], ys=de_normed_test_data[\"LATITUDE\"], \n",
        "                zs=de_normed_test_data[\"MONTH\"], c=test_labels, cmap='RdYlBu_r', s=40, linewidths=0)\n",
        "\n",
        "cb_ax = fig.add_axes([0.93, 0.1, 0.02, 0.8])\n",
        "cbar = fig.colorbar(my_color_bar, cax=cb_ax)\n",
        "\n",
        "\n",
        "# -- Predicted Values --\n",
        "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
        "\n",
        "ax2.scatter(xs=de_normed_test_data[\"LONGITUDE\"], ys=de_normed_test_data[\"LATITUDE\"], \n",
        "                zs=de_normed_test_data[\"MONTH\"], c=test_predictions, cmap='RdYlBu_r', s=40, linewidths=0)\n",
        "\n",
        "ax2.set_title(\"Predicted\", fontsize=14)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-i-ZlTae539",
        "colab_type": "text"
      },
      "source": [
        "##Actual vs Predicted Temperature Values 2D Scatter\n",
        "###Set up the data\n",
        "Our data is set up in a large array of latitude, longitude, elevation, month and temperature. In this step we will pull out the test data by month so that we can graph each month separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNp-Kh2fDI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prep data for 2D graphs \n",
        "\n",
        "# -- JANUARY -- 01\n",
        "january_list = []\n",
        "lat_01=[]\n",
        "long_01=[]\n",
        "tmax_01=[] #actual\n",
        "tmax_01_p=[] #predicted\n",
        "\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 1.0:\n",
        "        january_list.append(k)\n",
        "       \n",
        "for i in january_list:\n",
        "  lat_01.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_01.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_01.append(test_labels[i])\n",
        "  tmax_01_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "\n",
        "# -- MARCH -- 03\n",
        "march_list = []\n",
        "lat_03=[]\n",
        "long_03=[]\n",
        "tmax_03=[]\n",
        "tmax_03_p=[]\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 3.0:\n",
        "        march_list.append(k)\n",
        "       \n",
        "for i in march_list:\n",
        "  lat_03.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_03.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_03.append(test_labels[i])\n",
        "  tmax_03_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "# -- MAY -- 05\n",
        "may_list = []\n",
        "lat_05=[]\n",
        "long_05=[]\n",
        "tmax_05=[]\n",
        "tmax_05_p=[]\n",
        "\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 5.0:\n",
        "        may_list.append(k)\n",
        "       \n",
        "for i in may_list:\n",
        "  lat_05.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_05.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_05.append(test_labels[i])\n",
        "  tmax_05_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "# -- JULY -- 07\n",
        "july_list = []\n",
        "lat_07=[]\n",
        "long_07=[]\n",
        "tmax_07=[]\n",
        "tmax_07_p=[]\n",
        "\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 7.0:\n",
        "        july_list.append(k)\n",
        "       \n",
        "for i in july_list:\n",
        "  lat_07.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_07.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_07.append(test_labels[i])\n",
        "  tmax_07_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "# -- SEPTEMBER -- 09\n",
        "sept_list = []\n",
        "lat_09=[]\n",
        "long_09=[]\n",
        "tmax_09=[]\n",
        "tmax_09_p=[]\n",
        "\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 9.0:\n",
        "        sept_list.append(k)\n",
        "       \n",
        "for i in sept_list:\n",
        "  lat_09.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_09.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_09.append(test_labels[i])\n",
        "  tmax_09_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "# -- NOVEMBER -- 11\n",
        "november_list = []\n",
        "lat_11=[]\n",
        "long_11=[]\n",
        "tmax_11=[]\n",
        "tmax_11_p=[]\n",
        "\n",
        "\n",
        "for key, value in de_normed_test_data.items():\n",
        "  if key == 'MONTH':\n",
        "    for k, v in value.items():\n",
        "      if v == 11.0:\n",
        "        november_list.append(k)\n",
        "       \n",
        "for i in november_list:\n",
        "  lat_11.append(de_normed_test_data[\"LATITUDE\"][i])\n",
        "  long_11.append(de_normed_test_data[\"LONGITUDE\"][i])\n",
        "  tmax_11.append(test_labels[i])\n",
        "  tmax_11_p.append(test_predictions_keysd.get(i))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJf7DjriOMJ",
        "colab_type": "text"
      },
      "source": [
        "###Graph multiple 2D scatter subplots "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wknE2vUdiVmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setup the master plot\n",
        "sns.set(style='darkgrid')\n",
        "fig, axs = plt.subplots(6, 2, figsize=(11,25))\n",
        "fig.suptitle(\"Temperature (C)\", fontsize=16, y=.93)\n",
        "\n",
        "# ---- Plot January ----\n",
        "axs[0,0].scatter(x=long_01, y=lat_01, c=tmax_01, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[0,0].axes.xaxis.set_ticklabels([])\n",
        "axs[0,0].set_title(\"January Actual\")\n",
        "\n",
        "\n",
        "\n",
        "axs[0,1].scatter(x=long_01, y=lat_01, c=tmax_01_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[0,1].axes.xaxis.set_ticklabels([])\n",
        "axs[0,1].axes.yaxis.set_ticklabels([])\n",
        "axs[0,1].set_title(\"January Predicted\")\n",
        "\n",
        "#this is a place holder var for the colorbar\n",
        "my_color_bar = axs[0,0].scatter(x=long_01, y=lat_01, c=tmax_01, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "\n",
        "# ---- Plot March ----\n",
        "axs[1,0].scatter(x=long_03, y=lat_03, c=tmax_03, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[1,0].axes.xaxis.set_ticklabels([])\n",
        "axs[1,0].set_title(\"March Actual\")\n",
        "                        \n",
        "axs[1,1].scatter(x=long_03, y=lat_03, c=tmax_03_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[1,1].axes.xaxis.set_ticklabels([])\n",
        "axs[1,1].axes.yaxis.set_ticklabels([])\n",
        "axs[1,1].set_title(\"March Predicted\")                        \n",
        "\n",
        "# ---- Plot May ----\n",
        "axs[2,0].scatter(x=long_05, y=lat_05, c=tmax_05, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[2,0].axes.xaxis.set_ticklabels([])\n",
        "axs[2,0].set_title(\"May Actual\")\n",
        "                        \n",
        "axs[2,1].scatter(x=long_05, y=lat_05, c=tmax_05_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[2,1].axes.xaxis.set_ticklabels([])\n",
        "axs[2,1].axes.yaxis.set_ticklabels([])\n",
        "axs[2,1].set_title(\"May Predicted\")                        \n",
        "\n",
        "# ---- Plot July ----\n",
        "axs[3,0].scatter(x=long_07, y=lat_07, c=tmax_07, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[3,0].axes.xaxis.set_ticklabels([])\n",
        "axs[3,0].set_title(\"July Actual\")\n",
        "                        \n",
        "axs[3,1].scatter(x=long_07, y=lat_07, c=tmax_07_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[3,1].axes.xaxis.set_ticklabels([])\n",
        "axs[3,1].axes.xaxis.set_ticklabels([])                        \n",
        "axs[3,1].set_title(\"July Predicted\")                        \n",
        "\n",
        "\n",
        "# ---- Plot September ----\n",
        "axs[4,0].scatter(x=long_09, y=lat_09, c=tmax_09, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[4,0].axes.xaxis.set_ticklabels([])                        \n",
        "axs[4,0].set_title(\"September Actual\")\n",
        "                        \n",
        "axs[4,1].scatter(x=long_09, y=lat_09, c=tmax_09_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[4,1].axes.xaxis.set_ticklabels([])  \n",
        "axs[4,1].axes.yaxis.set_ticklabels([])                        \n",
        "axs[4,1].set_title(\"September Predicted\")                        \n",
        "\n",
        "\n",
        "# ---- Plot November ----\n",
        "axs[5,0].scatter(x=long_11, y=lat_11, c=tmax_11, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[5,0].set_title(\"November Actual\")\n",
        "                        \n",
        "axs[5,1].scatter(x=long_11, y=lat_11, c=tmax_11_p, cmap='RdYlBu_r', s=40, vmin=-10, vmax=45, linewidths=0)\n",
        "axs[5,1].axes.yaxis.set_ticklabels([])\n",
        "axs[5,1].set_title(\"November Predicted\")                        \n",
        "\n",
        "# setup color bar\n",
        "cb_ax = fig.add_axes([0.93, 0.1, 0.02, 0.8])\n",
        "cbar = fig.colorbar(my_color_bar, cax=cb_ax)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZlu10qaku0",
        "colab_type": "text"
      },
      "source": [
        "#Error\n",
        "Next, let's look at the error for each data point as calculated by subtracting the actual minus the predicted value. Are some regions more prone to error than others?\n",
        "\n",
        "First, we will look at overall error to examine whether certain locations are more prone to error than others, then we will break it down by month to see if some time periods are more prone to error than others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT2_AXOLYmYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(style='darkgrid')\n",
        "plot = plt.scatter(x=de_normed_test_data[\"LONGITUDE\"], y=de_normed_test_data[\"LATITUDE\"], \n",
        "                c=(test_labels - test_predictions), cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "plt.colorbar(plot)\n",
        "\n",
        "plt.title(\"Error: Actual - Predicted values\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO8XKIqEnmeC",
        "colab_type": "text"
      },
      "source": [
        "##Prepare the data\n",
        "To examine the error by month, we need to create a list of the error for each data point for each month. We will use the list of points for each month created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOnIvqpXnvEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Error\n",
        "total_error = test_labels - test_predictions\n",
        "\n",
        "# ---- January error ----\n",
        "error_01 = []\n",
        "\n",
        "for i in january_list:\n",
        "  error_01.append(total_error[i])\n",
        "  \n",
        "# ---- March error ----\n",
        "error_03 = []\n",
        "\n",
        "for i in march_list:\n",
        "  error_03.append(total_error[i])\n",
        "  \n",
        "# ---- May error ----\n",
        "error_05 = []\n",
        "\n",
        "for i in may_list:\n",
        "  error_05.append(total_error[i])  \n",
        "  \n",
        "# ---- July error ----\n",
        "error_07 = []\n",
        "\n",
        "for i in july_list:\n",
        "  error_07.append(total_error[i]) \n",
        "  \n",
        "# ---- September error ----\n",
        "error_09 = []\n",
        "\n",
        "for i in sept_list:\n",
        "  error_09.append(total_error[i])  \n",
        "  \n",
        "# ---- November error ----\n",
        "error_11 = []\n",
        "\n",
        "for i in november_list:\n",
        "  error_11.append(total_error[i])  \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFBOl9pShTCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up the master plot\n",
        "fig, axs = plt.subplots(3, 2, figsize=(11,15))\n",
        "fig.suptitle(\"Error in Degrees (C)\", fontsize=16)\n",
        "\n",
        "\n",
        "# ---- Plot January error----\n",
        "axs[0,0].scatter(x=long_01, y=lat_01, c=error_01, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "axs[0,0].axes.xaxis.set_ticklabels([])\n",
        "axs[0,0].set_title(\"January: Actual - Predicted\")\n",
        "\n",
        "\n",
        "#this is a place holder var for the colorbar\n",
        "my_color_bar = plt.scatter(x=de_normed_test_data[\"LONGITUDE\"], y=de_normed_test_data[\"LATITUDE\"], \n",
        "                c=(test_labels - test_predictions), cmap='BrBG', s=90)\n",
        "\n",
        "# ---- Plot March error ----\n",
        "axs[0,1].scatter(x=long_03, y=lat_03, c=error_03, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "axs[0,1].axes.xaxis.set_ticklabels([])\n",
        "axs[0,1].axes.yaxis.set_ticklabels([])\n",
        "axs[0,1].set_title(\"March: Actual - Predicted\")                                             \n",
        "\n",
        "# ---- Plot May error ----\n",
        "axs[1,0].scatter(x=long_05, y=lat_05, c=error_05, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "axs[1,0].axes.xaxis.set_ticklabels([])\n",
        "axs[1,0].set_title(\"May: Actual - Predicted\")                       \n",
        "\n",
        "# ---- Plot July error ----\n",
        "axs[1,1].scatter(x=long_07, y=lat_07, c=error_07, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "axs[1,1].axes.xaxis.set_ticklabels([])\n",
        "axs[1,1].axes.yaxis.set_ticklabels([])\n",
        "axs[1,1].set_title(\"July: Actual - Predicted\")                      \n",
        "\n",
        "# ---- Plot September error ----\n",
        "axs[2,0].scatter(x=long_09, y=lat_09, c=error_09, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)                        \n",
        "axs[2,0].set_title(\"September: Actual - Predicted\")\n",
        "\n",
        "# ---- Plot November ----\n",
        "axs[2,1].scatter(x=long_11, y=lat_11, c=error_11, cmap='BrBG', s=40, linewidths=0, vmin=-6, vmax=6)\n",
        "axs[2,1].axes.yaxis.set_ticklabels([])\n",
        "axs[2,1].set_title(\"November: Actual - Predicted\")                      \n",
        "\n",
        "# setup color bar\n",
        "cb_ax = fig.add_axes([0.93, 0.1, 0.02, 0.8])\n",
        "cbar = fig.colorbar(my_color_bar, cax=cb_ax)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u_-dD-yd5l8",
        "colab_type": "text"
      },
      "source": [
        "It looks like our model predicts reasonably well. Let's take a look at the error distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVa7MG4Nd6rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [TMAX]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEgeBiNKeFGh",
        "colab_type": "text"
      },
      "source": [
        "It is gaussian, which is encouraging!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLQOpf53eGk_",
        "colab_type": "text"
      },
      "source": [
        "#Conclusion\n",
        "\n",
        "This notebook introduced a few techniques to handle a regression problem.\n",
        "\n",
        "\n",
        "*    Mean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\n",
        "*   Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n",
        "*   When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
        "*   This basic neural network model works well with large and small data sets.\n",
        "*   Early stopping is a useful technique to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ]
    }
  ]
}